{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport nltk\\nnltk.download('stopwords')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import svm\n",
    "\n",
    "import scipy\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(directory):\n",
    "    df = pd.read_csv(directory,sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def get_final_csv(ids, y, filename):\n",
    "    pd.DataFrame(dict(Id = ids,Predicted = y)).to_csv(filename,sep=\",\",index=False)\n",
    "    \n",
    "def preprocessingV1(X_d,X_e):\n",
    "    \n",
    "    X_d = X_d.drop(columns=[\"region_2\",\"description\"])\n",
    "    X_e = X_e.drop(columns=[\"region_2\",\"description\"])\n",
    "\n",
    "    #imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    #X_d[\"country\"] = imputer.fit_transform(np.array(X_d[\"country\"]).reshape(-1,1))\n",
    "    #X_d[\"province\"] = imputer.fit_transform(np.array(X_d[\"province\"]).reshape(-1,1))\n",
    "    \n",
    "    X_d.fillna(\"other\", inplace=True)\n",
    "    X_e.fillna(\"other\", inplace=True)\n",
    "    \n",
    "    y = X_d.quality\n",
    "    X_d = X_d.drop(columns=[\"quality\"])\n",
    "    \n",
    "    df = pd.concat([X_d,X_e])\n",
    "    \n",
    "    df_enc = pd.get_dummies(df)\n",
    "    df_enc_scipy = scipy.sparse.csr_matrix(df_enc.values)\n",
    "    \n",
    "    return df_enc_scipy[:X_d.shape[0]], y, df_enc_scipy[X_d.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56000, 9), (56000,), (14000, 8), (14000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d = loadData('Dataset/dev.tsv')\n",
    "\n",
    "X_train = X_d[:56000].copy()\n",
    "X_test = X_d[56000:70000].copy()\n",
    "\n",
    "y_train = X_train.quality\n",
    "y_test = X_test.quality\n",
    "\n",
    "X_test.drop(columns=[\"quality\"],inplace=True)\n",
    "\n",
    "description_train = X_train[[\"description\"]]\n",
    "description_test = X_test[[\"description\"]]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d_prep, y, X_e_prep = preprocessingV1(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## naive\n",
    "#######################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7181166491338944"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_d_prep,y_train)\n",
    "\n",
    "y_pred = lr.predict(X_e_prep)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTIMENT\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "other_sw = ['anywh', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh','abov', \n",
    "            'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', \n",
    "            'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', \n",
    "            'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', \n",
    "            'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', \n",
    "            'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', \n",
    "            'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', \n",
    "            'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', \n",
    "            'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', \n",
    "            'wherev', 'whi', 'yourselv']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc).union(other_sw)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    output_str = re.sub('[^A-Za-zàèéìòù]+', ' ', text)\n",
    "    output_str_lower =re.sub('[A-Z]+', lambda m: m.group(0).lower(), output_str)\n",
    "    output_noss = ' '.join(w for w in output_str_lower.split() if w not in stop_words) \n",
    "    output_stemming = [stemmer.stem(i) for i in output_noss.split()]\n",
    "    output_stemming = ' '.join(output_stemming)\n",
    "    return output_stemming\n",
    "\n",
    "## get sentiment\n",
    "import numpy as np\n",
    "def getSentiment(X):\n",
    "    mean = np.mean(X[\"quality\"])\n",
    "    X[\"sentiment\"] = X[\"quality\"].apply(lambda x : 0 if x < mean else 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_train = X_train[[\"description\"]].copy()\n",
    "description_test = X_test[[\"description\"]].copy()\n",
    "\n",
    "description_train['description_cleaned'] = description_train['description'].apply(lambda x: clean_text(x)) \n",
    "description_test['description_cleaned'] = description_test['description'].apply(lambda x: clean_text(x)) \n",
    "\n",
    "description_train.drop(columns=[\"description\"],inplace=True)\n",
    "description_test.drop(columns=[\"description\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"min_df\": [3, 4], \n",
    "  \"max_df\": [0.5],\n",
    "  \"ngram_range\": [(1,3)]\n",
    "}\n",
    "\n",
    "for config in ParameterGrid(params):\n",
    "    pipe = Pipeline([('count', CountVectorizer(**config,  token_pattern=r'\\b[^\\d\\W]+\\b' )), ('tfid', TfidfTransformer())])\n",
    "    vectors = pipe.fit_transform(description_train.description_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56000, 91247)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_x = pipe.transform(description_test.description_cleaned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 91247)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "train_supremo = hstack((X_d_prep, vectors))\n",
    "test_supremo = hstack((X_e_prep, ev_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56000, 126545), (14000, 126545))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_supremo.shape, test_supremo.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(train_supremo,y_train)\n",
    "\n",
    "y_pred = lr.predict(test_supremo)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
