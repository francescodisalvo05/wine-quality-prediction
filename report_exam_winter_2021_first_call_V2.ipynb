{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first-call.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E1JxmXho0Z2t",
        "Za4RvxTk0dZl",
        "ZO2uzywj2x6N",
        "7VLyI12T0kqF",
        "lQzyxovM0n1o",
        "9Ku3J1Yn0v2k",
        "v8HCvdnP01Ff"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1JxmXho0Z2t"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfJqEvSl0bnr"
      },
      "source": [
        "from scipy.sparse import hstack\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.linear_model import LinearRegression,SGDRegressor\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "import scipy\r\n",
        "\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import spacy\r\n",
        "\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\r\n",
        "from sklearn.model_selection import ParameterGrid\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "sns.set_style(\"whitegrid\")\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4RvxTk0dZl"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqreR5mU0fEl"
      },
      "source": [
        "def loadData(directory):\r\n",
        "    df = pd.read_csv(directory,sep=\"\\t\")\r\n",
        "    return df\r\n",
        "\r\n",
        "def get_final_csv(ids, y, filename):\r\n",
        "    pd.DataFrame(dict(Id = ids,Predicted = y)).to_csv(filename,sep=\",\",index=False)\r\n",
        "    \r\n",
        "def evaluateModels(models, targets,X,y):\r\n",
        "    \r\n",
        "    scores = pd.DataFrame()\r\n",
        "    for model,target in zip(models,targets):\r\n",
        "        scores[target] = cross_val_score(model, X, y, scoring='r2', cv=3, n_jobs=-1)\r\n",
        "        \r\n",
        "    return scores\r\n",
        "\r\n",
        "##################################  \r\n",
        "## Encoding and Missing values\r\n",
        "##################################\r\n",
        "def preprocessing(X_d,X_e):\r\n",
        "    \r\n",
        "    X_d = X_d.drop(columns=[\"region_2\",\"description\"])\r\n",
        "    X_e = X_e.drop(columns=[\"region_2\",\"description\"])\r\n",
        "\r\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\r\n",
        "    X_d[\"country\"] = imputer.fit_transform(np.array(X_d[\"country\"]).reshape(-1,1))\r\n",
        "    X_d[\"province\"] = imputer.fit_transform(np.array(X_d[\"province\"]).reshape(-1,1))\r\n",
        "    \r\n",
        "    X_d = X_d.fillna(\"other\")\r\n",
        "    X_e = X_e.fillna(\"other\")\r\n",
        "    \r\n",
        "    y = X_d.quality\r\n",
        "    X_d = X_d.drop(columns=[\"quality\"])\r\n",
        "    \r\n",
        "    df = pd.concat([X_d,X_e])\r\n",
        "    \r\n",
        "    df_enc = pd.get_dummies(df)\r\n",
        "    df_enc_scipy = scipy.sparse.csr_matrix(df_enc.values)\r\n",
        "    \r\n",
        "    return df_enc_scipy[:X_d.shape[0]], y, df_enc_scipy[X_d.shape[0]:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMX-JT2E0ixF"
      },
      "source": [
        "##################################  \r\n",
        "## Document preprocessing\r\n",
        "##################################\r\n",
        "\r\n",
        "stemmer = SnowballStemmer('english')\r\n",
        "\r\n",
        "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\r\n",
        "other_sw = ['anywh', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh','abov', \r\n",
        "            'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', \r\n",
        "            'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', \r\n",
        "            'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', \r\n",
        "            'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', \r\n",
        "            'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', \r\n",
        "            'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', \r\n",
        "            'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', \r\n",
        "            'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', \r\n",
        "            'wherev', 'whi', 'yourselv']\r\n",
        "\r\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(punc).union(other_sw)\r\n",
        "\r\n",
        "\r\n",
        "def cleanDescriptions(text):\r\n",
        "    t = re.sub('[^A-Za-zàèéìòù]+', ' ', text)\r\n",
        "    t = re.sub('[A-Z]+', lambda m: m.group(0).lower(), t)\r\n",
        "    t = ' '.join(w for w in t.split() if w not in stop_words) \r\n",
        "    t = [stemmer.stem(i) for i in t.split()]\r\n",
        "    t = ' '.join(output_stemming)\r\n",
        "    return output_stemming\r\n",
        "\r\n",
        "def preprocessText(description_train,description_test):\r\n",
        "\r\n",
        "  # extract the descriptions\r\n",
        "  description_train = X_dev[[\"description\"]].copy()\r\n",
        "  description_test = X_eval[[\"description\"]].copy()\r\n",
        "\r\n",
        "  # clean the descriptions\r\n",
        "  description_train['description_cleaned'] = description_train['description'].apply(lambda x: preprocessText(x)) \r\n",
        "  description_test['description_cleaned'] = description_test['description'].apply(lambda x: preprocessText(x)) \r\n",
        "  description_train.drop(columns=[\"description\"],inplace=True)\r\n",
        "  description_test.drop(columns=[\"description\"],inplace=True)\r\n",
        "\r\n",
        "  ## POI ELIMINARE - SOLO PER REPORT\r\n",
        "  params = {\r\n",
        "    \"min_df\": [1], \r\n",
        "    \"max_df\": [0.4],\r\n",
        "    \"ngram_range\": [(1,4)]\r\n",
        "  }\r\n",
        "\r\n",
        "  for config in ParameterGrid(params):\r\n",
        "      pipe = Pipeline([\r\n",
        "                ('count', CountVectorizer(**config,  token_pattern=r'\\b[^\\d\\W]+\\b' )), \r\n",
        "                ('tfid', TfidfTransformer())\r\n",
        "      ])\r\n",
        "      \r\n",
        "      dev_vec = pipe.fit_transform(description_train.description_cleaned)\r\n",
        "\r\n",
        "  eval_vec = pipe.transform(description_test.description_cleaned)\r\n",
        "\r\n",
        "  return dev_vec, eval_vec\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO2uzywj2x6N"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COOrVKVW20Mh"
      },
      "source": [
        "# load datasets\r\n",
        "X_dev = loadData('dev.tsv')\r\n",
        "X_eval = loadData('eval.tsv')\r\n",
        "\r\n",
        "# drop duplicates\r\n",
        "X_dev = X_dev.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VLyI12T0kqF"
      },
      "source": [
        "## Preprocessing α "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUhfMhow0m99"
      },
      "source": [
        "####################################################\r\n",
        "## Preprocessing - removing the outliers : α\r\n",
        "####################################################\r\n",
        "\r\n",
        "# 1.5(IQR) Rule for detecting the outliers thresholds\r\n",
        "t = X_dev[\"quality\"].quantile(0.75) - X_dev[\"quality\"].quantile(0.25)\r\n",
        "min_t = X_dev[\"quality\"].quantile(0.25) - 1.5 * t\r\n",
        "max_t = X_dev[\"quality\"].quantile(0.75) + 1.5 * t\r\n",
        "\r\n",
        "# filter\r\n",
        "X_d_filtered = X_dev[X_dev[\"quality\"] >= min_t]\r\n",
        "X_d_filtered = X_d_filtered[X_d_filtered[\"quality\"] <= max_t]\r\n",
        "\r\n",
        "# encode and handle np.nan\r\n",
        "X_dev_prep_f, y_f, X_eval_prep_f = preprocessing(X_d_filtered,X_eval)\r\n",
        "\r\n",
        "# preprocess the descriptions\r\n",
        "dev_vec_f, eval_vec_f = preprocessText(X_d_filtered[[\"description\"]].copy(),\r\n",
        "                                   X_eval[[\"description\"]].copy())\r\n",
        "\r\n",
        "# concat the encoded df and the tf-idf\r\n",
        "X_conc_dev_f = hstack((X_dev_prep, dev_vec))\r\n",
        "X_conc_eval_f = hstack((X_eval_prep, eval_vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQzyxovM0n1o"
      },
      "source": [
        "## Preprocessing β"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZYYFdpo0p4A"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "####################################################\r\n",
        "## Preprocessing - without removing the outliers : β\r\n",
        "####################################################\r\n",
        "\r\n",
        "# encode and handle np.nan\r\n",
        "X_dev_prep, y, X_eval_prep = preprocessing(X_dev,X_eval)\r\n",
        "\r\n",
        "# preprocess the descriptions\r\n",
        "dev_vec, eval_vec = preprocessText(X_dev[[\"description\"]].copy(),\r\n",
        "                                   X_eval[[\"description\"]].copy())\r\n",
        "\r\n",
        "# concat the encoded df and the tf-idf\r\n",
        "X_conc_dev = hstack((X_dev_prep, dev_vec))\r\n",
        "X_conc_eval = hstack((X_eval_prep, eval_vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0CSaDOa0uWN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ku3J1Yn0v2k"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCiGLB0G0xpb"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "####################################################\r\n",
        "## Evaliuate : α\r\n",
        "####################################################\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\r\n",
        "\r\n",
        "models = [LinearRegression(),SGDRegressor()]\r\n",
        "targets = [\"LinearRegression\",\"SGDRegressor\"]\r\n",
        "\r\n",
        "scores_f = evaluateModels(models,targets,X_conc_dev_f,y_f)\r\n",
        "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(scores), ax=ax,palette=\"OrRd_r\")\r\n",
        "plt.xlabel(\"Model\")\r\n",
        "plt.ylabel(\"R2_Score\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ38gk1f3Dhh"
      },
      "source": [
        "####################################################\r\n",
        "## Evaluate : β\r\n",
        "####################################################\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\r\n",
        "\r\n",
        "models = [LinearRegression(),,SGDRegressor()]\r\n",
        "targets = [\"LinearRegression\",\"SGDRegressor\"]\r\n",
        "\r\n",
        "scores = evaluateModels(models,targets,X_conc_dev,y)\r\n",
        "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(scores), ax=ax,palette=\"OrRd_r\")\r\n",
        "plt.xlabel(\"Model\")\r\n",
        "plt.ylabel(\"R2_Score\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8HCvdnP01Ff"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQYqLvh802tk"
      },
      "source": [
        "def doGridSearch(model,hyperparams,X,y):\r\n",
        "    \r\n",
        "    gs = GridSearchCV(estimator=model,  \r\n",
        "                         param_grid=hyperparams,\r\n",
        "                         scoring='r2',\r\n",
        "                         cv=3,\r\n",
        "                         n_jobs=4,\r\n",
        "                         verbose=True)\r\n",
        "\r\n",
        "    gs.fit(X, y)\r\n",
        "    \r\n",
        "    return gs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctcyQxJM1CRj"
      },
      "source": [
        "####################################################\r\n",
        "## Grid search linear regression\r\n",
        "####################################################\r\n",
        "\r\n",
        "hyperparams_LR = {\r\n",
        "    'fit_intercept' : [True,False],\r\n",
        "    'normalize' : [True,False]\r\n",
        "}\r\n",
        "\r\n",
        "gs_lr = doGridSearch(LinearRegression(), \"LinearRegression\",hyperparams_LR,train_supremo,y)\r\n",
        "\r\n",
        "print(f\"Best params:\\t{gs_lr.best_params_}\")\r\n",
        "print(f\"Best score:\\t{gs_lr.best_score_}\")\r\n",
        "\r\n",
        "y_pred_lr = gs_lr.predict(test_supremo)\r\n",
        "get_final_csv(list(X_eval.index),y_pred_lr,\"submit-linear-regression.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmCXOY691Dbf"
      },
      "source": [
        "####################################################\r\n",
        "## Grid search SGD Regressor\r\n",
        "####################################################\r\n",
        "\r\n",
        "hyperparams_SGD = {\r\n",
        "    'loss' : ['squared_loss','huber'],\r\n",
        "    'penalty' : ['l1','l2',None],\r\n",
        "    'alpha' : np.logspace(-5, 0, 6),\r\n",
        "    'eta0' : [0.01, 0.1]\r\n",
        "}\r\n",
        "\r\n",
        "gs_sgd = doGridSearch(SGDRegressor(max_iter=10000), \"SGDRegressor\",hyperparams_SGD,train_supremo,y)\r\n",
        "print(f\"Best params:\\t{gs_sgd.best_params_}\")\r\n",
        "print(f\"Best score:\\t{gs_sgd.best_score_}\")\r\n",
        "\r\n",
        "y_pred_sgd = gs_sgd.predict(test_supremo)\r\n",
        "get_final_csv(list(X_eval.index),y_pred_sgd,\"submit-sgd-regressor.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhPUDfAy1E5v"
      },
      "source": [
        "## Final prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpPwY-ad1FXa"
      },
      "source": [
        "## model used for the final score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}